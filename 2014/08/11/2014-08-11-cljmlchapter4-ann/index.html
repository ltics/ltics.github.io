<!DOCTYPE html>
<html lang=en>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="在这一章中，我们将会介绍人工神经网络(ANNs)。我们将会学习ANNs的基本知识和概念并且对可以解决监督学习和非监督学习的几个ANN模型进行讨论，最后会介绍Enclog这个clojure的库去构建ANNS。 神经网络非常适合于从给定的数据集中寻找一种特定的模式，并且有许多实际应用，比如说手写字识别和计算机视觉。ANNs通常通常是以一种融合的方式去对给定的问题进行建模寻找模式。有意思的是，神经网络可">
<meta property="og:type" content="article">
<meta property="og:title" content="cljmlchapter4-ann">
<meta property="og:url" content="http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/index.html">
<meta property="og:site_name" content="">
<meta property="og:description" content="在这一章中，我们将会介绍人工神经网络(ANNs)。我们将会学习ANNs的基本知识和概念并且对可以解决监督学习和非监督学习的几个ANN模型进行讨论，最后会介绍Enclog这个clojure的库去构建ANNS。 神经网络非常适合于从给定的数据集中寻找一种特定的模式，并且有许多实际应用，比如说手写字识别和计算机视觉。ANNs通常通常是以一种融合的方式去对给定的问题进行建模寻找模式。有意思的是，神经网络可">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/images/cljml/chap4/clojureformlchap4pic1.png">
<meta property="og:image" content="http://yoursite.com/images/cljml/chap4/clojureformlchap4pic2.png">
<meta property="og:image" content="http://yoursite.com/images/cljml/chap4/clojureformlchap4pic3.png">
<meta property="og:image" content="http://yoursite.com/images/cljml/chap4/clojureformlchap4pic4.png">
<meta property="og:image" content="http://yoursite.com/images/cljml/chap4/clojureformlchap4pic5.png">
<meta property="og:image" content="http://yoursite.com/images/cljml/chap4/clojureformlchap4pic6.png">
<meta property="og:image" content="http://yoursite.com/images/cljml/chap4/clojureformlchap4pic7.png">
<meta property="og:image" content="http://yoursite.com/images/cljml/chap4/clojureformlchap4pic8.png">
<meta property="og:image" content="http://yoursite.com/images/cljml/chap4/clojureformlchap4pic9.png">
<meta property="article:published_time" content="2014-08-11T15:04:15.000Z">
<meta property="article:modified_time" content="2020-05-05T11:48:09.279Z">
<meta property="article:author" content="Zheng Jihui">
<meta property="article:tag" content="Clojure">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/images/cljml/chap4/clojureformlchap4pic1.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>cljmlchapter4-ann</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body class="max-width mx-auto px3 ltr">
    
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="http://zjhmale.github.io" target="_blank" rel="noopener">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="http://github.com/zjhmale" target="_blank" rel="noopener">Projects</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2014/08/13/2014-08-13-cljmlchapter5-select-data/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2014/08/08/2014-08-08-cljmlchapter3-category-data/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/" target="_blank" rel="noopener"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&text=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&title=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&is_video=false&description=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=cljmlchapter4-ann&body=Check out this article: http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&title=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&title=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&title=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&title=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&name=cljmlchapter4-ann&description=" target="_blank" rel="noopener"><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://news.ycombinator.com/submitlink?u=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&t=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#理解非线性回归"><span class="toc-number">1.</span> <span class="toc-text">理解非线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#描述神经网络"><span class="toc-number">2.</span> <span class="toc-text">描述神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#理解多层感知器神经网络"><span class="toc-number">3.</span> <span class="toc-text">理解多层感知器神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#理解反向传播算法"><span class="toc-number">4.</span> <span class="toc-text">理解反向传播算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#理解递归神经网络"><span class="toc-number">5.</span> <span class="toc-text">理解递归神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#建立自组织神经网络"><span class="toc-number">6.</span> <span class="toc-text">建立自组织神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#本章概要"><span class="toc-number">7.</span> <span class="toc-text">本章概要</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        cljmlchapter4-ann
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name"></span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2014-08-11T15:04:15.000Z" itemprop="datePublished">2014-08-11</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/Clojure/" rel="tag">Clojure</a>, <a class="tag-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>在这一章中，我们将会介绍<strong>人工神经网络(ANNs)</strong>。我们将会学习ANNs的基本知识和概念并且对可以解决监督学习和非监督学习的几个ANN模型进行讨论，最后会介绍<strong>Enclog</strong>这个clojure的库去构建ANNS。</p>
<p>神经网络非常适合于从给定的数据集中寻找一种特定的模式，并且有许多实际应用，比如说手写字识别和计算机视觉。ANNs通常通常是以一种融合的方式去对给定的问题进行建模寻找模式。有意思的是，神经网络可以被应用在多种机器学习问题上，比如回归和分类问题。ANNs在计算机科学的其他领域都有广泛的应用，而不单单是局限在机器学习的研究上。</p>
<p><strong>非监督学习</strong>是机器学习中的一种形式，此类问题中给定的训练数据集中并没有标注训练样本的输出是属于哪一个类别。由于训练数据集是<em>非标注</em>的，所以一个非监督学习算法必须完全靠它自己去确定每一个样本的输出所对应的类别。通常情况下，非监督学习算法会寻找训练样本之间的相似性，然后将它们分组归类到几个不同的类别中。这样的技术通常也叫<strong>聚类分析</strong>，在后面的章节中我们会更深入的学习这种方法论。ANNs被用在非监督学习领域更多的是得益于其在非标注数据集中能快速发现特征的能力。这中由ANNs表现出来的非监督学习的特殊形式也成为<strong>竞争学习</strong>。</p>
<p>关于人工神经网络的一个有趣的事实是，它们是由高等生物中表现出学习能力的中枢神经系统的结构和行为建模得到的。</p>
<h2 id="理解非线性回归"><a href="#理解非线性回归" class="headerlink" title="理解非线性回归"></a>理解非线性回归</h2><p>目前为止，读着必须知道的一个事实就是在使用线性回归和逻辑斯蒂回归解决回归和分类问题时可以使用梯度下降算法来估计参数。随之而来的一个问题就是，既然我们已经可以使用梯度下降算法和训练数据来估计调整线性回归和逻辑斯蒂回归模型的参数，那我们为什么还需要神经网络。要理解为什么需要神经网络，我们首先需要理解<em>非线性回归</em>。</p>
<p>让我们假设现在有一个单特征变量X和一个因变量Y，Y随着X的变化曲线如下图所示</p>
<center>
    <img src="/images/cljml/chap4/clojureformlchap4pic1.png">
</center>

<p>如上图所示，使用线性等式非常难甚至是不可能对因变量Y和自变量X之间的关系进行建模。我们只能用高阶多项式才能对Y和X之间的关系进行建模，从而将问题转化为线性回归的标准形式。因此因变量Y和自变量X之间的关系是非线性的，在Y和X的关系式中存在X的高阶项。当然，也很有可能连高阶多项式都没有办法建模去拟合Y和X之间复杂的非线性关系。</p>
<p>可以看到使用梯度下降算法去更新一个多项式函数中的所有权重或者系数的值带来的时间复杂度是<script type="math/tex">O\left ( n^{2} \right )</script>，其中n是训练数据集中的特征个数。计算一个三阶多项式所有项的系数的算法复杂度是<script type="math/tex">O\left ( n^{3} \right )</script>。可以看到梯度下降算法的时间复杂度随着模型中的特征数的增多以几何级增长。因此梯度下降算法在对拥有大量特征非线性回归问题建模时非常的低效。</p>
<p>而ANNs，在对拥有高维度特征的数据集进行非线性回归建模时十分高效。我们将会学习ANNs中的一些基础概念以及几个可以应用在监督学习和非监督学习问题上的ANN模型。</p>
<h2 id="描述神经网络"><a href="#描述神经网络" class="headerlink" title="描述神经网络"></a>描述神经网络</h2><p>ANNs是根据生物体，比如哺乳动物和爬行动物中拥有学习能力的中枢神经系统的行为来进行建模的。这些生物体的中枢神经系统包括生物体的大脑，脊髓和支持神经组织的网络。大脑处理信息并且产生电信号，并将这些电信号通过神经纤维组成的网络传输到生物体中不同的器官上。尽管生物体的大脑需要进行非常复杂的计算和控制任务，但是它实际上只是神经元的一个集合。对应生物体感官信号的处理实际上也是由这些神经元组成的多个复杂的网络来进行的。当然，每个神经元只能处理由大脑处理的信息中的非常小的一部分。大脑的功能其实是将不同感知器官产生的电信号通过由神经元组成的复杂网络路由到运动器官上。一个独立的神经元有如下图所示的细胞结构。</p>
<center>
    <img src="/images/cljml/chap4/clojureformlchap4pic2.png">
</center>

<p>一个神经元有有多个靠近细胞核的树突和一个用来传递神经元细胞核信号的轴突。树突通常是被用来接受从其他神经元中发出的信号，并且将这些接收到得信号作为当前神经元的输入信号。类似的，神经元的轴突就相当于是神经元的输出。因此一个神经元可以被数学化地描述成为一个接收多个参数作为输入并且有一个输出的函数。</p>
<p>神经元之间通常都是相互连接的，这些神经元连接起来形成的网络就被称为<strong>神经网络</strong>。一个神经元本质上就是接收微弱的电信号，然后再将电信号传递给其他的神经元。两个神经元之间相互连接的空间也叫做<strong>突触</strong>。</p>
<p>ANN由多个相互连接的神经元连接组成。每一个神经元都可以被抽象成一个具有多个输入和单个输出的数学函数，如下图所示：</p>
<center>
    <img src="/images/cljml/chap4/clojureformlchap4pic3.png">
</center>

<p>单个的神经元可以被上面的图像所描述。从数学的角度上来说，只是一个简单的函数<script type="math/tex">\hat{y}</script>，这个函数将多个输入值<script type="math/tex">\left ( x_{1}, x_{2} ... x_{m} \right )</script>映射到一个输出<script type="math/tex">y</script>，<script type="math/tex">\hat{y}</script>函数就被称为是<strong>激活函数</strong>。神经元在这种表示情况下也被称作<strong>感知器</strong>。感知器可以被单独使用，并且足以对一些监督学习模型比如线性回归和逻辑斯蒂回归进行模拟和评估。当然，复杂的非线性数据最好还是使用多个相互连接的感知器来进行建模。</p>
<p>通常情况下，有一个常数值会作为偏差项作为感知器的一维输入，对于输入<script type="math/tex">\left ( x_{1}, x_{2} ... x_{m} \right )</script>，我们添加一个<script type="math/tex">x_{0}</script>作为偏差输入，我们另<script type="math/tex">x_{0} = 1</script>。一个加上了偏差输入的神经元可以被表示成下图所示的样子：</p>
<center>
    <img src="/images/cljml/chap4/clojureformlchap4pic4.png">
</center>

<p>对于感知器的每一个输入<script type="math/tex">x_{i}</script>，都有一个对应的权值<script type="math/tex">w_{i}</script>。这个权值和线性回归模型中每一个特征对应的系数很类似。因此激活函数就是一个关于输入值及其对应的权值的一个函数。我们可以形式化通过输入，权值以及感知器的激活函数来定义感知器这个预计输出<script type="math/tex">\hat{y}</script>，如下面的公式所示：</p>
<script type="math/tex; mode=display">\hat{y} = g\left ( \sum_{i=0}^{m}x_{i}\cdot w_{i} \right )</script><p>其中</p>
<script type="math/tex; mode=display">g\left ( x, w \right )</script><p>为激活函数</p>
<p>一个ANN节点所使用的激活函数很大程度上取决于待建模的训练数据。通常来说，<strong>sigmoid</strong>或者<strong>双曲正切</strong>函数会在分类问题上被用于作为激活函数(更多内容可以参考论文<em>Wavelet Neural Network (WNN) approach for calibration model building based on gasoline near infrared (NIR) spectr</em>)。sigmoid函数是否激活取决于输入是否能达到阈值。我们可以画出sigmoid函数的变化曲线来描述这种行为，如下图所示：</p>
<center>
    <img src="/images/cljml/chap4/clojureformlchap4pic5.png">
</center>

<p>ANNs可以大致被分为<em>前馈神经网络</em>和<em>递归神经网络</em>(更多内容可以参考论文<em>Bidirectional recurrent neural networks</em>)。这两种神经网络的区别在于，前馈神经网络不会形成一个有向循环，而递归神经网络则会通过节点之间的连接形成一个有向环。因此前馈神经网络每一个节点只能从节点所在层的上一层接受输入。有许多神经网络模型都有实际的应用，我们将会在接下来的章节中对其中的一些进行探讨。</p>
<h2 id="理解多层感知器神经网络"><a href="#理解多层感知器神经网络" class="headerlink" title="理解多层感知器神经网络"></a>理解多层感知器神经网络</h2><p>现在我们来了解一种简单的前馈神经网络模型-<strong>多层感知器</strong>模型。这个模型展示了前馈神经网络基本的样子并且在监督学习领域去对回归和分类问题进行建模拥有足够的通用性。在前馈神经网络中所有的输入都是单向流动的。这也是为什么在前馈神经网络中的任意一个层级上都没有<em>反馈</em>存在。<br>这里的反馈指的是在神经网络中的某一个给定层的输出又会作为输入反向作用在先前层级的感知器上。使用单层的感知器意味着只有一个激活函数，从而和使用<em>逻辑斯蒂回归</em>去对给定的训练数据进行建模有着相同的效果。这意味着这个模型无法去对非线性数据进行建模，而这也正是我们为什么需要ANNs。一定要注意，我们在<em>第三章，对数据分类</em>中已经讨论过了逻辑斯蒂回归。</p>
<p>一个多层感知器人工神经网络可以用下图来直观的描述：</p>
<center>
    <img src="/images/cljml/chap4/clojureformlchap4pic6.png">
</center>

<p>一个多层感知器神经网络是由许多层感知器节点组成的。如上图所示，有一个输入层，有一个输出层和若干隐含层，而每一层中又是由若干感知器组成的。输出层接收输入值，然后利用每个输入对应的权值以及激活函数计算出新的输出，并将这些新的值传递给下一个隐含层。</p>
<p>训练数据集中的每一个样本都可以被表示成<script type="math/tex">\left ( y^{i}, x^{i} \right )</script>，其中<script type="math/tex">y^{i}</script>表示的第i个样本的期望输出，<script type="math/tex">x^{i}</script>表示的是第i个样本的输入。<script type="math/tex">x^{i}</script>其实是一个长度为训练数据中特征数量列向量。</p>
<p>每一个感知器节点的输出都可以被称为是这个节点的激活值，第l层第i个节点的激活值就被表示成为<script type="math/tex">a_{i}^{\left ( l \right )}</script>。正如之前提到的用于计算激活值的激活函数通常选用sigmoid函数或者双曲正切函数。当然任何其他的数学函数都可以作为激活函数去拟合特定的训练数据。多层感知器网络的输入层加上了一个偏移量到输入向量中，作为神经网络的最终输入，并且后面一层的输入值就是之前一层的激活值。可以用下面的等式表示这种关系：</p>
<script type="math/tex; mode=display">a_{i}^{\left ( l \right )} = x_{i}</script><p>神经网络的每一对前后图层之间都有一个相对应的权重矩阵。这些权重矩阵的列数和靠近输入层的层级中节点的个数相同，行数和靠近输出层的层级中节点的个数相同，对于第l层来说，权重矩阵可以被表示成：</p>
<script type="math/tex; mode=display">W^{\left ( l \right )}</script><p>神经网络第l层的激活值也可以使用激活函数计算出来。将权值矩阵和上一层的激活值向量相乘得到的结果作为传入激活函数。通常情况下，在多层感知器模型中使用sigmoid函数作为激活函数。上面的过程可以使用下面的公式来表示：</p>
<script type="math/tex; mode=display">a^{\left ( l \right )} = g\left ( W^{\left ( l \right )}\cdot a^{\left ( l-1 \right )} \right )</script><p>其中</p>
<script type="math/tex; mode=display">g\left ( x \right )</script><p>是激活函数。</p>
<p>通常情况下，多层感知器神经网络使用sigmoid函数作为激活函数。需要注意的是我们没有在输出层增加一个偏移量。同样的，输出层可以产生任意的输出值。为了对一个<em>k分类</em>分类问题进行建模，神经网络需要产生<em>k</em>个输出值。</p>
<p>对于二分类问题，我们仅仅需要对一个至多只有两个类别的输入数据进行建模。ANN产生的输出不是0就是1。因此对于<em>k=2</em>的分类问题，<script type="math/tex">y \in \left \{ 0, 1 \right \}</script>。</p>
<p>同样地对于多分类问题，可以使用<em>k</em>个二分类的输出去模拟，所以输出值是一个<script type="math/tex">k \cdot 1</script>的矩阵，如下所示：</p>
<script type="math/tex; mode=display">y_{c = 1} = \begin{bmatrix}
1\\ 
0\\ 
\vdots\\
0
\end{bmatrix}
,
y_{c = 2} = \begin{bmatrix}
0\\ 
1\\ 
\vdots\\ 
0
\end{bmatrix}
,
\ldots
y_{c=k} =\begin{bmatrix}
0\\ 
0\\ 
\vdots\\ 
1
\end{bmatrix}
, 其中 \; \left | y \right | = k\cdot 1</script><p>至此，我们可以使用多层感知器神经网络去解决二分类和多分类问题。在后面的章节中我们将会学习并且实现<strong>反向传播算法</strong>来训练多层感知器神经网络。</p>
<p>架设我们现在要多异或门的行为进行建模。异或门可以被想象成是一个具有两个输入和一个输出的而分类器。如下图所示是一个对异或门进行建模的神经网络的结构。有趣的是，线性回归可以被用于对与门和或门进行建模但是却没有办法对异或门进行建模。这是因为异或门的输出是非线性性质的，而神经网络却可以很有效的解决这个问题。</p>
<center>
    <img src="/images/cljml/chap4/clojureformlchap4pic7.png">
</center>

<p>上图显示的多层感知器的输入层有三个节点，隐含层有四个节点，输入层有一个节点。可以看到除了输出层，每一层都加上了一个偏移输入。这个神经网络有两个突触，并且对应的有两个权值矩阵<script type="math/tex">W^{\left ( 1 \right )}</script>和<script type="math/tex">W^{\left ( 2 \right )}</script>，注意到第一个突触是在输入层和隐含层之间，第二个突触是在隐含层和输出层之间。所以权值矩阵<script type="math/tex">W^{\left ( 1 \right )}</script>是一个<script type="math/tex">3\cdot3</script>的矩阵，而权值矩阵<script type="math/tex">W^{\left ( 2 \right )}</script>是一个<script type="math/tex">1\cdot4</script>的矩阵。<script type="math/tex">W</script>也通常被用来表示神经网络中的所有权值。</p>
<p>既然已经使用sigmoid函数作为多层感知器神经网络每个节点的激活函数，我们可以参照逻辑斯蒂回归模型类似的定义一个关于每一个节点权值的损失函数。神经网络的损失函数可以被定义成是以权值矩阵为变量的函数，如下所示：</p>
<script type="math/tex; mode=display">J\left ( W \right ) = -\frac{1}{N}\cdot \left ( \sum_{i=1}^{N}\cdot \sum_{k=1}^{K}y_{k}^{\left ( i \right )}\cdot log( \hat{y}\cdot ( x^{(i)})_{k})+(1-y_{k}^{(i)})\cdot log(1-\hat{y}(x^{(i)})_{k}) \right )</script><p>上面所示的损失函数本质上是神经网络输出层上每一个节点的损失函数的平均值(更多细节参考论文<em>Neural Networks in Materials Science</em>)。对于一个有<script type="math/tex">K</script>个输出的多层感知器神经网络来说，我们多这<script type="math/tex">K</script>个因子的求平均。需要注意的是<script type="math/tex">y_{k}^{(i)}</script>表示神经网络的第<script type="math/tex">K</script>个输出，<script type="math/tex">x^{(i)}</script>表示神经网络的输入，<script type="math/tex">N</script>表示训练数据集中样本的数量。这个损失函数本质上是在量化一个有<script type="math/tex">K</script>个输出的逻辑斯蒂回归。我们可以给之前的损失函数增加一个正则化系数来表示一个正则化损失函数，如下所示：</p>
<script type="math/tex; mode=display">J\left ( W \right ) = -\frac{1}{N}\cdot \left ( \sum_{i=1}^{N}\cdot \sum_{k=1}^{K}y_{k}^{\left ( i \right )}\cdot log( \hat{y}\cdot( x^{(i)})_{k})+(1-y_{k}^{(i)})\cdot log(1-\hat{y}(x^{(i)})_{k}) \right ) + \frac{\lambda }{2N}\cdot \sum_{l=1}^{L-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l+1}}(W_{ij}^{(l)})^{2}</script><p>上面加上了正则化因子的损失函数和逻辑斯蒂回归中的正则化损失函数也非常的类似。正则化因子本质上是神经网络中所有权值的平方和，但是不包括偏移输入的权值。<script type="math/tex">s_{l}</script>表示的是神经网络中第<script type="math/tex">l</script>层的节点的个数。有趣的是在上面的正则化损失函数中，只有正则化因子是依赖于神经网络的层数的。因此一个神经网络预估模型的<em>泛化能力</em>是基于这个神经网络的层数的。</p>
<h2 id="理解反向传播算法"><a href="#理解反向传播算法" class="headerlink" title="理解反向传播算法"></a>理解反向传播算法</h2><p><strong>反向传播学习算法</strong>利用给定的训练集来对多层感知器神经网络进行训练。简单的来说，这个算法首先利用给定的输入值来计算输出值，然后计算神经网络的输出误差量。神经网络的输出误差量是对比真实输出和训练数据集中的期望输出来计算得到的。再利用计算出来的误差来更新神经网络的权值。因此当用一定量的样本对神经网络训练完之后，这个神经网络将有能力为给定的输入预测出输出值。这个算法包含独立的三个步骤，如下所示：</p>
<ul>
<li>前向传播阶段</li>
<li>后向传播阶段</li>
<li>更新权值阶段</li>
</ul>
<p>神经网络突触上的权值矩阵中的每一个元素首先初始化成一个小范围内接近零的随机数，这个范围表示为<script type="math/tex">[-\epsilon, \epsilon]</script>。在这个范围内随机地初始化权值是为了避免权值矩阵出现对称的情况，为了避免对称采取的行为叫做<strong>对成型破缺</strong>，这是为了让神经网络的突触上的每一个权值在每一次用反向传播算法进行迭代的时候能够产生明显的改变。这也描述了在神经网络中每一感知器节点都是相互独立地进行学习。假如所有节点都有相同的权值，那么训练学习出来的预估模型就很有可能是欠拟合的活着是过拟合的。</p>
<p>此外，反向传播学习算法还需要两个额外的参数，一个是学习速率<script type="math/tex">\rho</script>，另一个是学习动量<script type="math/tex">\Lambda</script>，在后面章节的例子中我们将会看到这两个参数的作用。</p>
<p>前向传播阶段只是简单的计算了神经网络每一层中所有节点的激活值。正如之前提到的神经网络输入层节点的激活值就是神经网络的输入值和输入偏移量，可以被形式化地定义为下面这个等式：</p>
<script type="math/tex; mode=display">a_{i}^{(l)} = x_{i}, 其中 \; l = 1</script><p>利用神经网络输入层的激活值，其他层节点的激活值也就随之确定了。这个过程是利用前一层的激活值和权值的乘积和传入激活函数，然后计算出当前层的激活值，可以使用下面的公式：</p>
<script type="math/tex; mode=display">a^{(l)} = g(W^{(l)}\cdot a^{(l-1)})</script><p>从上面的公式中可以看到第<script type="math/tex">l</script>层的激活值是由前一层的激活值以及对应的权值的乘积作为激活函数的参数计算得到的。之后输出层的激活值就会被反向传播了，意思是，神经网络这些激活值将会通过隐含层从输出层反向传播到输入层。在这一个步骤中，我们需要计算神经网络每一个节点的误差量。输出层的误差值是利用期望的输出值<script type="math/tex">y^{(i)}</script>和输出层的激活值<script type="math/tex">a^{(L)}</script>的差值计算出来的，可以用下面的等式表示：</p>
<script type="math/tex; mode=display">\delta ^{(L)} = a^{(L)} - y^{(i)}</script><script type="math/tex; mode=display">\delta^{(l)}$$这一项表示的是第$$l$$层的误差向量，是一个$$j\cdot1$$的列向量，其中j是这一层中节点的个数，这一项可以被定义为如下所示的形式：

$$ \delta ^{(l)} = \left [ \delta_{j}^{(l)} \right ], 其中 \;j \in \left \{ nodes \; in \; layer \; l \right \}</script><p>神经网络中除了输出层的其他层的误差项使用下面的公式计算：</p>
<script type="math/tex; mode=display">\delta ^{(l)} = (W^{(l)})^{T}\cdot \delta ^{(l+1)}.*g^{'}(a^{(l)}), 其中 \; g^{'}(a^{(l)})=a^{(l)}.*(1-a^{(l))})</script><p>在上面的等式中，<script type="math/tex">.*</script>这个符号用来代表相同形状的矩阵逐元素乘法的操作行文，需要注意的是这和矩阵相乘的操作不同，逐元素乘法会返回一个矩阵，这个矩阵中每一个元素都是由相乘的两个形状相同的矩阵对应位置的元素相乘得到的，所以做乘法的两个矩阵和计算得到的矩阵都有相同的形状，可以用下面的等式描述这种操作：</p>
<script type="math/tex; mode=display">c_{ij} = a_{ij} \cdot b_{ij}</script><p>而<script type="math/tex">g^{'}(x)</script>这一项则表示的是神经网络中使用的激活函数的导，由于我们现在使用的是sigmoid函数，所以<script type="math/tex">g^{'}(a^{(l)})</script>其实就是<script type="math/tex">a^{(l)}.*(1-a^{(l)})</script>。</p>
<p>因此，我们现在可以计算神经网络中所有节点的误差值了。我们可以利用这些误差值去确定神经网络突触上的梯度。我们现在进入反向传播算法最后更新权值的这一步骤。<br>各个突触的梯度矩阵最初的时候都是使用<script type="math/tex">0</script>去作为所有元素的初始值。突触的梯度矩阵和其对应的权值矩阵具有相同的形状。<script type="math/tex">\Delta ^{(l)}</script>被用来表示神经网络中第<script type="math/tex">l</script>层之后的突触的梯度矩阵，神经网络突触梯度矩阵的初始值通常为如下所示的形式：</p>
<script type="math/tex; mode=display">\Delta ^{(l)} = \begin{bmatrix}
0
\end{bmatrix}</script><p>对于训练数据集中的每一个样本，我们都会计算神经网络中每一个节点的误差和激活值。这些值会被加到突触的梯度矩阵中，如下所示：</p>
<script type="math/tex; mode=display">\Delta ^{(l)} := \Delta ^{(l)} + \delta ^{l+1}\cdot\left(a^{(l)}\right)^{T}</script><p>然后计算矩阵矩阵的平均值，就是除以样本集的大小。然后利用误差矩阵和梯度矩阵去更新每一层的权值矩阵，如下面的表达式所示：</p>
<script type="math/tex; mode=display">W^{(l)} := W^{(l)} - \left(\rho \cdot \Delta ^{(l)} + \Lambda \cdot \delta^{(l)}\right)</script><p>因此，反向传播算法中的学习速率和学习动量这两个参数只是在更新权重这个步骤中用到了。注意上面那个公式中的<script type="math/tex">\delta^{(l)}</script>并不是指当前层的误差矩阵，而是当前层之前一次训练得到的权值变化量矩阵，动量项的意义也就在于要在上一次的权值变化方向上保持一定的惯性。之前提到的这三个公式就是反向传播算法中单次迭代的所有步骤了，需要进行大量的迭代更新操作直到神经网络的总误差收敛到一个很小的值。现在我们可以总结一下反向传播学习算法的所有步骤了：</p>
<ol>
<li>使用很小范围内的随机数初始化神经网络突触的全职矩阵。</li>
<li>选取一个样本并且使用前向传播方法计算出神经网络中每一个感知器节点的激活值。</li>
<li>从输出层到输入层通过隐含层反向传播最后的输出层的激活值，这一步中，将需要计算神经网络每一个节点的误差</li>
<li>利用第三步计算得到的误差矩阵和突触的输入激活矩阵相乘得到神经网络中每一个节点的权重的梯度，每一个梯度都被表示成比率或者是百分比。</li>
<li>利用梯度矩阵和误差矩阵计算神经网络中每一个突触权重矩阵的变化量，然后对于的权重矩阵再去减去这个变化量，这就是反向传播算法更新权值的核心步骤了。</li>
<li>对剩下的样本数据重复步骤2至步骤5。</li>
</ol>
<p>反向传播学习算法有许多独立的部分，我们会分别实现它们，并最终组成一个完整的算法实现。因为神经网络中突触的误差和权值以及激活值都可以被表现为矩阵形式，所以我们可以使用矩阵操作实现这个算法。</p>
<blockquote>
<p>注意接下来的例子中，我们需要Incanter库中<code>incanter.core</code>这个名字空间里的函数。事实上在这个名字空间中的函数使用的是Clatrix这个库来表示矩阵和封装矩阵操作。</p>
</blockquote>
<p>让我们架设我们现在需要实现使用神经网络去对异或门的逻辑行为进行建模。训练数据就只是简单的异或门的真值表，并且如下所示表示成向量的形式：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">;;定义训练数据集</span></span><br><span class="line">(<span class="name"><span class="builtin-name">def</span></span> sample-data [[[<span class="number">0</span> <span class="number">0</span>] [<span class="number">0</span>]]</span><br><span class="line">                  [[<span class="number">0</span> <span class="number">1</span>] [<span class="number">1</span>]]</span><br><span class="line">                  [[<span class="number">1</span> <span class="number">0</span>] [<span class="number">1</span>]]</span><br><span class="line">                  [[<span class="number">1</span> <span class="number">1</span>] [<span class="number">0</span>]]])</span><br></pre></td></tr></table></figure>
<p>上面定义的向量<code>sample-data</code>中的每一个元素本身又是由异或门的输入向量和输出向量组成的。我们会利用这个向量形式的训练数据集来训练构建我们的神经网络模型。预测与非门的输出本质上是一个分类问题，我们将会使用神经网络对这个问题进行建模。从抽象意义上来说，一个神经网络需要具备解决二分类和多分类问题的能力，所以我们现在定义一个神经网络的抽象结构，如下面代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defprotocol</span></span> NeuralNetwork</span><br><span class="line">  (<span class="name">run</span>        [network inputs])</span><br><span class="line">  (<span class="name">run-binary</span> [network inputs])</span><br><span class="line">  (<span class="name">train-ann</span>  [network samples]))</span><br></pre></td></tr></table></figure>
<p>上面代码定义的<code>NeuralNetwork</code>协议中有三个函数。<code>train-ann</code>函数被用来训练神经网络，并且需要一些训练样本数据。<code>run</code>和<code>run-binary</code>函数可以被用来解决多分类和二分类问题，两个函数都需要输入值。</p>
<p>反向传播算法的第一步是初始化神经网络每个突触对应的权值矩阵。我们可以使用<code>rand</code>和<code>matrix</code>函数去产生这些权值矩阵，如下面代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> rand-list</span><br><span class="line">  <span class="string">"创建一个随机的双精度浮点数列表，其中每一个元素的取值范围在[-epsilon, epsilon]内。"</span></span><br><span class="line">  [len epsilon]</span><br><span class="line">  (<span class="name"><span class="builtin-name">map</span></span> (<span class="name"><span class="builtin-name">fn</span></span> [x] (<span class="name"><span class="builtin-name">-</span></span> (<span class="name"><span class="builtin-name">rand</span></span> (<span class="name"><span class="builtin-name">*</span></span> <span class="number">2</span> epsilon)) epsilon))</span><br><span class="line">       (<span class="name"><span class="builtin-name">range</span></span> <span class="number">0</span> len)))</span><br></pre></td></tr></table></figure>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> random-initial-weights</span><br><span class="line">  <span class="string">"对于给定的层级产生一个初识随机权值矩阵，layers参数中必须是由层级中节点数组成的一个向量。"</span></span><br><span class="line">  [layers epsilon]</span><br><span class="line">  (<span class="name"><span class="builtin-name">for</span></span> [i (<span class="name"><span class="builtin-name">range</span></span> <span class="number">0</span> (<span class="name"><span class="builtin-name">dec</span></span> (<span class="name">length</span> layers)))]</span><br><span class="line">    (<span class="name"><span class="builtin-name">let</span></span> [cols (<span class="name"><span class="builtin-name">inc</span></span> (<span class="name"><span class="builtin-name">get</span></span> layers i))     <span class="comment">; 列的个数是输入的个数+1，因为要算上偏移量输入</span></span><br><span class="line">          rows (<span class="name"><span class="builtin-name">get</span></span> layers (<span class="name"><span class="builtin-name">inc</span></span> i))]    <span class="comment">; 行的个数是下一层感知器节点的个数</span></span><br><span class="line">      (<span class="name">matrix</span> (<span class="name">rand-list</span> (<span class="name"><span class="builtin-name">*</span></span> rows cols) epsilon) cols))))</span><br></pre></td></tr></table></figure>
<p>上面代码所示的<code>rand-list</code>函数创建了一个随机列表，其中的每一个元素的取值范围都在<script type="math/tex">[-\epsilon, \epsilon]</script>之间。如之前解释过的，我们需要以此来破坏权值矩阵的对称性。</p>
<p><code>random-initial-weights</code>函数会为神经网络不同突触产生对应的权值矩阵。如上面代码定义的那样，<code>layers</code>参数必须是一个由神经网络各个层级中节点数量组成的向量。假如一个神经网络在输入层有两个节点，隐含层有三个节点，输出层有一个节点，就需要把<code>[2 3 1]</code>当做<code>layers</code>参数传给<code>random-initial-weights</code>函数。每一个权值矩阵的列数都和输入的个数相同，行数和下一层节点的个数相同，这里在输入中加上了一个额外的偏移量输入。需要注意的是我们使用了<code>matrix</code>函数的另一种形式，这种形式的第一个参数是一个向量，第二个参数是一个数值，目标输出的矩阵的列数是由第二个参数确定的，然后去将第一个参数表示的向量划分成一个矩阵。因此，作为第一个参数传入的向量必须有<code>(* rows cols)</code>个元素，其中<code>rows</code>和<code>cols</code>分别是权值矩阵的行数和列数。</p>
<p>因为对神经网络中的每一个节点我们都需要用sigmoid函数去计算激活值，所以我们必须定义一个函数，去对传入给定的输出矩阵中的每一个元素使用sigmoid函数得到激活值矩阵。我们可以使用<code>incanter.core</code>名字空间里的<code>div</code>，<code>plus</code>，<code>exp</code>和<code>minus</code>函数去实现这个功能，如下面的代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> sigmoid</span><br><span class="line">  <span class="string">"对于传入的矩阵z，将其作为1/(1+exp(-z))函数的参数得到一个新的矩阵。"</span></span><br><span class="line">  [z]</span><br><span class="line">  (<span class="name">div</span> <span class="number">1</span> (<span class="name">plus</span> <span class="number">1</span> (<span class="name">exp</span> (<span class="name">minus</span> z)))))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意之前定义过的所有函数中包含的算数运算都是作用在给定矩阵的所有元素上的，并且返回一个新的矩阵。</p>
</blockquote>
<p>我们还必须隐式地向神经网络中得各层增加一个偏移量。可以通过实现一个<code>bind-rows</code>函数来封装这个操作，这个函数会对给定的矩阵增加一行元素，如下面的代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> bind-bias</span><br><span class="line">  <span class="string">"在传入的向量上增加一个偏移量输入。"</span></span><br><span class="line">  [v]</span><br><span class="line">  (<span class="name">bind-rows</span> [<span class="number">1</span>] v))</span><br></pre></td></tr></table></figure>
<p>因为偏移量的数值一般都是<script type="math/tex">1</script>，所以我们在<code>bind-bias</code>函数中指定添加上的行元素为<code>[1]</code>。</p>
<p>使用之前定义的函数，我们就可以实现前向传播过程了。这个过程本质上是将神经网络两个层级之间的突出的权值和前一个层级产生的激活值相乘然后传入sigmoid函数中得到下一个层级的激活值，如下面的代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> matrix-mult</span><br><span class="line">  <span class="string">"将传入的两个元素相乘，其中可能有元素是矩阵，并且保证函数返回的结果也一定是一个矩阵"</span></span><br><span class="line">  [a b]</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [result (<span class="name">mmult</span> a b)]</span><br><span class="line">    (<span class="name"><span class="builtin-name">if</span></span> (<span class="name">matrix?</span> result)</span><br><span class="line">      result</span><br><span class="line">      (<span class="name">matrix</span> [result]))))</span><br></pre></td></tr></table></figure>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> forward-propagate-layer</span><br><span class="line">  <span class="string">"利用第l层和第l+1层之间突触对应的权值矩阵以及第l层的激活值矩阵计算第l+1层的激活值矩阵。"</span></span><br><span class="line">  [weights activations]</span><br><span class="line">  (<span class="name">sigmoid</span> (<span class="name">matrix-mult</span> weights activations)))</span><br></pre></td></tr></table></figure>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> forward-propagate</span><br><span class="line">  <span class="string">"通过神经网络的权值矩阵前向传播激活值矩阵，并且将最终输出层的激活值返回。"</span></span><br><span class="line">  [weights input-activations]</span><br><span class="line">  (<span class="name"><span class="builtin-name">reduce</span></span> #(<span class="name">forward-propagate-layer</span> %<span class="number">2</span> (<span class="name">bind-bias</span> %<span class="number">1</span>))</span><br><span class="line">          input-activations weights))</span><br></pre></td></tr></table></figure>
<p>在上面的代码中，我们先定义了一个<code>matrix-mult</code>函数，这个函数可以将连个矩阵相乘，并且保证返回的也是一个矩阵。注意在<code>matrix-mult</code>函数中我们使用<code>mmult</code>而不是<code>mult</code>函数，因为<code>mult</code>函数是对两个相同形状的矩阵做逐元素乘法。</p>
<p>使用<code>matrix-mult</code>函数和<code>sigmoid</code>函数，我们可以实现神经网络中两个层级之间的前向传播步骤。这个步骤最终是在<code>forward-propagate-layer</code>函数中实现的，在这个函数中仅仅是将神经网络中两个层之间的突出对应的权值矩阵和输入的激活值矩阵相乘并且保证返回的一定是一个矩阵。在利用一组输入值在神经网络的所有层级之间前向传播的过程中我们必须在使用<code>forward-propagate-layer</code>函数为每一个层级增加一个偏移量输入，在上面代码定义的<code>forward-propagate</code>函数中就是用了将<code>forward-propagate-layer</code>函数放在<code>reduce</code>函数的闭包中来优雅的实现这个功能。</p>
<p>尽管<code>forward-propagate</code>函数可以确定神经网络的输出层激活值，但是我们仍然需要神经网络中其他节点的激活值来进行方向传播步骤。我们可以将reduce操作变成另一种递归操作，并且申明一个容器变量来存储神经网络中每一层的激活值矩阵。下面代码中定义的<code>forward-propagate-all-activations</code>函数就利用一个loop形式去递归的调用<code>forward-propagate-layer</code>函数从而实现获得每一个层级激活值矩阵的想法：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> forward-propagate-all-activations</span><br><span class="line">  <span class="string">"在神经网络中前向传播激活值矩阵，并且返回所有节点的激活值。"</span></span><br><span class="line">  [weights input-activations]</span><br><span class="line">  (<span class="name"><span class="builtin-name">loop</span></span> [all-weights     weights</span><br><span class="line">         <span class="comment">;;列向量上再加上一行</span></span><br><span class="line">         activations     (<span class="name">bind-bias</span> input-activations)</span><br><span class="line">         all-activations [activations]]</span><br><span class="line">    (<span class="name"><span class="builtin-name">let</span></span> [[weights</span><br><span class="line">           &amp; all-weights'] all-weights</span><br><span class="line">           last-iter?       (<span class="name"><span class="builtin-name">empty?</span></span> all-weights')</span><br><span class="line">           out-activations  (<span class="name">forward-propagate-layer</span></span><br><span class="line">                             weights activations)</span><br><span class="line">           activations'     (<span class="name"><span class="builtin-name">if</span></span> last-iter? out-activations</span><br><span class="line">                                (<span class="name">bind-bias</span> out-activations))</span><br><span class="line">           all-activations' (<span class="name"><span class="builtin-name">conj</span></span> all-activations activations')]</span><br><span class="line">      (<span class="name"><span class="builtin-name">if</span></span> last-iter? all-activations'</span><br><span class="line">          (<span class="name"><span class="builtin-name">recur</span></span> all-weights' activations' all-activations')))))</span><br></pre></td></tr></table></figure>
<p>上面代码定义的<code>forward-propagate-all-activations</code>函数需要神经网络所有节点的权值，并且将输入值当做初识激活值传入此函数。我们首先用了<code>bind-bias</code>函数在神经网络输入矩阵上加入了偏移量输入。然后我们用一个叫<code>all-activations</code>的容器去存放每一次产生的激活值矩阵，这个容器实际上时一个向量。然后<code>forward-propagate-layer</code>函数会作用在神经网络每一层的权值矩阵上，并且每一次迭代时都会在相关层级的输入上加上一个偏移量输入。</p>
<blockquote>
<p>注意我们不会在最后一次迭代也就是计算到神经网络输出层的时候加上偏移量输入。因此<code>forward-propagate-all-activations</code><br>函数会在前向传播过程中作用在神经网络每一层的节点上并且得到相应的激活值。要注意的是在<code>all-activations</code>这个向量中激活值矩阵的顺序是和神经网络中层级的顺序一致的。</p>
</blockquote>
<p>现在我们来实现反向传播学习算法的反向传播步骤。首先我们需要实现利用公式<script type="math/tex">\delta ^{(l)} = \left ( W^{(l)} \right )^{T}\cdot \delta^{(l+1)} .*\left ( a^{(l)}.*\left ( 1-a^{(l)} \right ) \right )</script>来计算误差项<script type="math/tex">\delta^{(l)}</script>的函数，如下面代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> back-propagate-layer</span><br><span class="line">  <span class="string">"将第l+1层的误差反向传播计算并返回第l层的误差。"</span></span><br><span class="line">  [deltas weights layer-activations]</span><br><span class="line">  (<span class="name">mult</span> (<span class="name">matrix-mult</span> (<span class="name">trans</span> weights) deltas)</span><br><span class="line">        (<span class="name">mult</span> layer-activations (<span class="name">minus</span> <span class="number">1</span> layer-activations))))</span><br></pre></td></tr></table></figure>
<p>上面代码定义的<code>back-propagate-layer</code>函数利用神经网络第l层和第l+1层之间突触对应的权值矩阵以及第l+1层的误差来计算第l层的误差。</p>
<blockquote>
<p>注意到我们仅仅是在用<code>matrix-mult</code>函数计算<script type="math/tex">\left ( W^{(l)} \right )^{T}\cdot \delta^{(l+1)}</script>这一项的时候用到了矩阵乘法操作，另外其他的相乘操作都是利用<code>mult</code>函数进行的矩阵间逐元素相乘操作。</p>
</blockquote>
<p>本质上来说，我们需要将上面的函数从输出层通过隐含层作用至输入层从而计算神经网络中每一个节点的误差值。这些误差值再作用到节点的激活之上，以此来产生来更新神经网络节点权值的梯度值。我们可以用一个和<code>forward-propagate-all-activations</code>函数类似的函数去递归将<code>back-propagate-layer</code>函数作用在神经网络的不同层上，以此来实现权值更新的行为。当然我们必须逆向地穿过神经网络的各个层级，也就是说从输出层开始，通过隐含层直到输入层，我们用下面的代码来实现：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> calc-deltas</span><br><span class="line">  <span class="string">"利用反向传播计算并返回神经网络中的所有误差包括输出层的误差。"</span></span><br><span class="line">  [weights activations output-deltas]</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [hidden-weights     (<span class="name"><span class="builtin-name">reverse</span></span> (<span class="name"><span class="builtin-name">rest</span></span> weights))</span><br><span class="line">        hidden-activations (<span class="name"><span class="builtin-name">rest</span></span> (<span class="name"><span class="builtin-name">reverse</span></span> (<span class="name"><span class="builtin-name">rest</span></span> activations)))]</span><br><span class="line">    (<span class="name"><span class="builtin-name">loop</span></span> [deltas          output-deltas</span><br><span class="line">           all-weights     hidden-weights</span><br><span class="line">           all-activations hidden-activations</span><br><span class="line">           all-deltas      (<span class="name"><span class="builtin-name">list</span></span> output-deltas)]</span><br><span class="line">      (<span class="name"><span class="builtin-name">if</span></span> (<span class="name"><span class="builtin-name">empty?</span></span> all-weights) all-deltas</span><br><span class="line">        (<span class="name"><span class="builtin-name">let</span></span> [[weights</span><br><span class="line">               &amp; all-weights']      all-weights</span><br><span class="line">               [activations</span><br><span class="line">                &amp; all-activations'] all-activations</span><br><span class="line">              deltas'        (<span class="name">back-propagate-layer</span></span><br><span class="line">                               deltas weights activations)</span><br><span class="line">              all-deltas'    (<span class="name"><span class="builtin-name">cons</span></span> (<span class="name"><span class="builtin-name">rest</span></span> deltas')</span><br><span class="line">                                   all-deltas)]</span><br><span class="line">          (<span class="name"><span class="builtin-name">recur</span></span> deltas' all-weights' all-activations' all-deltas'))))))</span><br></pre></td></tr></table></figure>
<p><code>calc-deltas</code>函数确定了神经网络中每一个感知器节点的误差。在这个计算过程中并不需要输入层和输出层的激活值。我们仅仅需要隐含层的激活值去计算误差值，并将激活值绑定在<code>hidden-activations</code>变量上。同样的我们也不需要输入层对应的权值，然后将去掉输入层权值的权值矩阵绑定在<code>hidden-weights</code>变量上。然后<code>calc-deltas</code>函数就会用<code>back-propagate-layer</code>函数作用在神经网络每一个突触上的权值矩阵上，然后就可以以矩阵的形式得到所有节点的误差值了。需要注意的是我们没有将偏移量节点的误差加入到最终的结果集中，我们使用<code>rest</code>函数，<code>(rest deltas&#39;)</code>，作用在给定突触层的误差结果集上，因为结果集中的第一个误差是偏移量输入的误差，而当前层的偏移量与前一层是独立的，所以其误差也不需要向前一层反向传播。</p>
<p>根据之前的定义，突触层梯度向量这个因子<script type="math/tex">\Delta^{(l)}</script>是根据矩阵<script type="math/tex">\delta^{(l+1)}</script>和<script type="math/tex">a^{(l)}</script>相乘计算得到的，<script type="math/tex">\delta^{(l+1)}</script>表示后面一层的误差矩阵<script type="math/tex">a^{(l)}</script>表示当前给顶层级的激活值矩阵。我们可以用下面的代码实现计算梯度的过程：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> calc-gradients</span><br><span class="line">  <span class="string">"利用误差和激活值计算梯度值。"</span></span><br><span class="line">  [deltas activations]</span><br><span class="line">  (<span class="name"><span class="builtin-name">map</span></span> #(<span class="name">mmult</span> %<span class="number">1</span> (<span class="name">trans</span> %<span class="number">2</span>)) deltas activations))</span><br></pre></td></tr></table></figure>
<p>上面代码所示的<code>calc-gradients</code>函数优雅的实现了计算<script type="math/tex">\delta^{(l+1)}\cdot \left(a^{(l)}\right)^{T}</script>这一项。由于需要操作误差和激活值序列，我们使用<code>map</code>函数作用在之前等式所描述的神经网络中的误差矩阵和激活值矩阵。使用<code>calc-deltas</code>和<code>calc-gradient</code>函数，我们可以计算一个给定的训练样本在通过神经网络训练后隐含层每一个节点的误差以及对应层前一层的权重的梯度值，并且最终我们需要把这一次训练的输出误差平方和返回。可以使用下面的代码实现上述的过程：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> calc-error</span><br><span class="line">  <span class="string">"对于给定的权值矩阵计算误差矩阵和平方误差项。"</span></span><br><span class="line">  [weights [input expected-output]]</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [activations    (<span class="name">forward-propagate-all-activations</span></span><br><span class="line">                         weights (<span class="name">matrix</span> input))</span><br><span class="line">        output         (<span class="name"><span class="builtin-name">last</span></span> activations)</span><br><span class="line">        output-deltas  (<span class="name">minus</span> output expected-output)</span><br><span class="line">        all-deltas     (<span class="name">calc-deltas</span></span><br><span class="line">                         weights activations output-deltas)</span><br><span class="line">        gradients      (<span class="name">calc-gradients</span> all-deltas activations)]</span><br><span class="line">    (<span class="name"><span class="builtin-name">list</span></span> gradients</span><br><span class="line">          (<span class="name">sum</span> (<span class="name">pow</span> output-deltas <span class="number">2</span>)))))</span><br></pre></td></tr></table></figure>
<p>上面代码所定义的<code>calc-error</code>函数需要两个参数-神经网络突触对应的权值矩阵以及训练样本值，其中训练样本传入函数时会被解构成<code>[input expected]</code>的形式。首先会利用<code>forward-propagation-all-activations</code>函数来计算神经网络中所有节点的激活值，然后利用样本中的期望输出值和神经网络的实际输出值之间做差来计算输出层的误差值。神经网络计算出来的输出值，实际上就是利用<code>forward-propagate-all-activations</code>函数计算出来的激活值列表的最后一个元素，在上面的代码表示成<code>(last activations)</code>。使用计算出来的激活值矩阵和<code>calc-deltas</code>函数，就可以确定每一个感知器节点的误差值了。然后这些计算出来的误差值再被传入<code>calc-gradients</code>函数就可以用来确定神经网络前面层级权值的梯度值了。对于给定的样本值，神经网络通过将输出层的所有误差求平方和来计算<strong>均方误差(MSE)</strong>。</p>
<p>对于神经网络中给定的权值矩阵，我们必须先初始化与之对应的和权值矩阵形状相同的梯度矩阵，梯度矩阵中所有的元素都必须被初始化为<script type="math/tex">0</script>。我们可以使用<code>dim</code>函数和<code>matrix</code>函数的另一种用法来实现这个初始化过程，<code>dim</code>函数会以向量形式返回给定矩阵的形状，如下面的代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> new-gradient-matrix</span><br><span class="line">  <span class="string">"返回一个和给定的权值矩阵相同形状并且每一个元素都为0的矩阵。"</span></span><br><span class="line">  [weight-matrix]</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [[rows cols] (<span class="name">dim</span> weight-matrix)]</span><br><span class="line">    (<span class="name">matrix</span> <span class="number">0</span> rows cols)))</span><br></pre></td></tr></table></figure>
<p>在上面代码所定义的<code>new-gradient-matrix</code>函数中，<code>matrix</code>函数接受三个参数，一个表示每个元素内容的数值，行的大小以及列的大小去初始化一个矩阵。这个函数将会返回一个和给定权值矩阵相同形状的矩阵作为梯度矩阵的初始值。</p>
<p>我们现在来实现函数<code>calc-gradients-and-error</code>从而可以将<code>calc-error</code>函数作用在权值矩阵和输入样本值上。我们必须将<code>calc-error</code>函数作用在每一个训练样本上，并且累加每一次计算得到的梯度值和<strong>MSE</strong>值。然后我们计算这些累加值的平均值，并且返回与给定权值矩阵和样本集对应的取均值之后的梯度矩阵以及总的<strong>MSE</strong>值。我们用下面的代码来实现这个过程：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> calc-gradients-and-error' [weights samples]</span><br><span class="line">  (<span class="name"><span class="builtin-name">loop</span></span> [gradients   (<span class="name"><span class="builtin-name">map</span></span> new-gradient-matrix weights)</span><br><span class="line">         total-error <span class="number">1</span></span><br><span class="line">         samples     samples]</span><br><span class="line">    (<span class="name"><span class="builtin-name">let</span></span> [[sample</span><br><span class="line">           &amp; samples']     samples</span><br><span class="line">           [new-gradients</span><br><span class="line">            squared-error] (<span class="name">calc-error</span> weights sample)</span><br><span class="line">            gradients'        (<span class="name"><span class="builtin-name">map</span></span> plus new-gradients gradients)</span><br><span class="line">            total-error'   (<span class="name"><span class="builtin-name">+</span></span> total-error squared-error)]</span><br><span class="line">      (<span class="name"><span class="builtin-name">if</span></span> (<span class="name"><span class="builtin-name">empty?</span></span> samples')</span><br><span class="line">        (<span class="name"><span class="builtin-name">list</span></span> gradients' total-error')</span><br><span class="line">        (<span class="name"><span class="builtin-name">recur</span></span> gradients' total-error' samples')))))</span><br><span class="line">        </span><br><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> calc-gradients-and-error</span><br><span class="line">  <span class="string">"Calculate gradients and MSE for sample</span></span><br><span class="line"><span class="string">  set and weight matrix."</span></span><br><span class="line">  [weights samples]</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [num-samples   (<span class="name">length</span> samples)</span><br><span class="line">        [gradients</span><br><span class="line">         total-error] (<span class="name">calc-gradients-and-error'</span></span><br><span class="line">                        weights samples)]</span><br><span class="line">    (<span class="name"><span class="builtin-name">list</span></span></span><br><span class="line">      (<span class="name"><span class="builtin-name">map</span></span> #(<span class="name">div</span> % num-samples) gradients)    <span class="comment">; gradients</span></span><br><span class="line">      (/ total-error num-samples))))          <span class="comment">; MSE</span></span><br></pre></td></tr></table></figure>
<p>上面代码中定义的<code>calc-gradients-and-error</code>函数依赖于<code>calc-gradients-and-error&#39;</code>这个辅助函数。<code>calc-gradients-and-error&#39;</code>函数一开始初始化了梯度矩阵，然后应用了<code>calc-error</code>函数的功能，最后将计算得到的梯度值和<strong>MSE</strong>累加并返回。<code>calc-gradients-and-error</code>函数只是简单的将<code>calc-gradients-and-error&#39;</code>函数返回的权值矩阵和<strong>MSE</strong>处以样本的个数求取平均值。</p>
<p>现在，我们唯一没有实现的就是利用之前计算得到的梯度值去更新神经网络每一个节点的权值了。简单的来说，我们需要一直更新权重矩阵直到<strong>MSE</strong>能收敛到一个很小的值。这实际上是对神经网络中的每一个节点使用梯度下降算法。现在我们将会实现这个稍有不同的梯度下降算法从而可以通过连续不断地更新神经网络中节点的权值的方式来对神经网络进行训练，如下面的代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> gradient-descent-complete?</span><br><span class="line">  <span class="string">"判断梯度下降训练算法是否达到可以结束的条件。"</span></span><br><span class="line">  [network iter mse]</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [options (<span class="symbol">:options</span> network)]</span><br><span class="line">    (<span class="name"><span class="builtin-name">or</span></span> (<span class="name"><span class="builtin-name">&gt;=</span></span> iter (<span class="symbol">:max-iters</span> options))</span><br><span class="line">        (<span class="name"><span class="builtin-name">&lt;</span></span> mse (<span class="symbol">:desired-error</span> options)))))</span><br></pre></td></tr></table></figure>
<p>上面代码定义的<code>gradient-descent-complete?</code>函数只是简单地检查梯度下降算法是否达到了终止条件。这个函数假设使用<code>network</code>参数表示的神经网络是一个带有<code>:options</code>键的<code>map</code>或者<code>record</code>类型的对象。这个键对应的值将会保存有神经网络中所有的配置选项。<code>gradient-descent-complete?</code>函数会检查神经网络总的<strong>MSE</strong>是否小于期望值或者训练的迭代次数是否已经达到了最大值，这两个条件参数分别作为<code>:desire-error</code>键和<code>:max-iters</code>键的值存储在神经网络的配置选项中。</p>
<p>现在，我们将会为多层感知器神经网络实现<em>梯度下降</em>算法。在这个实现中，我们利用梯度下降算法提供的<em>step</em>函数来计算权值的变化量。计算出来的权值的变化量再被加到神经网络突触现有的权值上。为多层感知器神经网络实现<em>梯度下降</em>算法的代码如下所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> apply-weight-changes</span><br><span class="line">  <span class="string">"利用对应的变换量去更新权值矩阵。"</span></span><br><span class="line">  [weights changes]</span><br><span class="line">  (<span class="name"><span class="builtin-name">map</span></span> plus weights changes))</span><br><span class="line"></span><br><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> gradient-descent</span><br><span class="line">  <span class="string">"利用梯度下降去训练调整神经网络的权值"</span></span><br><span class="line">  [step-fn init-state network samples]</span><br><span class="line">  (<span class="name"><span class="builtin-name">loop</span></span> [network network</span><br><span class="line">         state init-state</span><br><span class="line">         iter <span class="number">0</span>]</span><br><span class="line">    (<span class="name"><span class="builtin-name">let</span></span> [iter     (<span class="name"><span class="builtin-name">inc</span></span> iter)</span><br><span class="line">          weights  (<span class="symbol">:weights</span> network)</span><br><span class="line">          [gradients</span><br><span class="line">           mse]    (<span class="name">calc-gradients-and-error</span> weights samples)]</span><br><span class="line">      (<span class="name"><span class="builtin-name">if</span></span> (<span class="name">gradient-descent-complete?</span> network iter mse)</span><br><span class="line">        network</span><br><span class="line">        (<span class="name"><span class="builtin-name">let</span></span> [[changes state] (<span class="name">step-fn</span> network gradients state)</span><br><span class="line">              new-weights     (<span class="name">apply-weight-changes</span> weights changes)</span><br><span class="line">              network         (<span class="name"><span class="builtin-name">assoc</span></span> network <span class="symbol">:weights</span> new-weights)]</span><br><span class="line">          (<span class="name"><span class="builtin-name">recur</span></span> network state iter))))))</span><br></pre></td></tr></table></figure>
<p>上面代码定义的<code>apply-weight-changes</code>函数只是简单的将神经网络中的权值矩阵和其对应的变化量相加。<code>gradient-descent</code>函数需要一个<code>step</code>函数(准确的来说是<code>step-fn</code>函数)，神经网络权值变化量矩阵的初识状态，神经网络自身，以及用来训练神经网络的训练数据集。这个函数每次计算神经网络训练时的梯度矩阵以及权值变化矩阵，其中计算权值变化矩阵是通过<code>step-in</code>函数来实现的。然后使用<code>apply-weight-changes</code>函数来更新神经网络的权值，重复迭代这个操作过程直到<code>gradient-descent-complete?</code>函数返回<code>true</code>。神经网络的权值矩阵可以被<code>network</code>这个map对象的<code>:weights</code>键索引到。更新权值矩阵也是用新的权值矩阵去替换掉原本与<code>:weights</code>键对应的值。在反向传播算法中，我们需要确定神经网络的学习速率和学习动量。这两个参数需要在计算权值变化值之前被确定下来。<code>step-fn</code>函数可以得到这两个参数进而可以计算权值的变化量，’step-fn’函数被当做参数传给<code>gradient-descent</code>函数，如下面的代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> calc-weight-changes</span><br><span class="line">  <span class="string">"计算权值的变化量:</span></span><br><span class="line"><span class="string">  changes = learning rate * gradients +</span></span><br><span class="line"><span class="string">            learning momentum * deltas"</span></span><br><span class="line">  [gradients deltas learning-rate learning-momentum]</span><br><span class="line">  (<span class="name"><span class="builtin-name">map</span></span> #(<span class="name">plus</span> (<span class="name">mult</span> learning-rate %<span class="number">1</span>)</span><br><span class="line">              (<span class="name">mult</span> learning-momentum %<span class="number">2</span>))</span><br><span class="line">       gradients deltas))</span><br><span class="line">       </span><br><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> bprop-step-fn [network gradients deltas]</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [options             (<span class="symbol">:options</span> network)</span><br><span class="line">        learning-rate       (<span class="symbol">:learning-rate</span> options)</span><br><span class="line">        learning-momentum   (<span class="symbol">:learning-momentum</span> options)</span><br><span class="line">        changes             (<span class="name">calc-weight-changes</span></span><br><span class="line">                              gradients deltas</span><br><span class="line">                              learning-rate learning-momentum)]</span><br><span class="line">    [(<span class="name"><span class="builtin-name">map</span></span> minus changes) changes]))</span><br><span class="line"></span><br><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> gradient-descent-bprop [network samples]</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [gradients (<span class="name"><span class="builtin-name">map</span></span> new-gradient-matrix (<span class="symbol">:weights</span> network))]</span><br><span class="line">    (<span class="name">gradient-descent</span> bprop-step-fn gradients</span><br><span class="line">                      network samples)))</span><br></pre></td></tr></table></figure>
<p>上面代码定义的<code>calc-weight-changes</code>函数利用神经网络给定层的梯度矩阵以及前一次计算得到的权值变化量矩阵计算这一次迭代权值的变化量，实际上就是上面公式提到的<script type="math/tex">\rho \cdot \Delta^{(l)} + \Lambda \cdot \delta^{(l)}</script>这一项。<code>bprop-step-fn</code>函数从用<code>network</code>表示成的神经网络对象中提取出学习速率和学习动量两个参数，然后调用<code>calc-weight-changes</code>函数。由于最终在<code>grandient-descent</code>函数中权值矩阵是会加上权值变化量矩阵，所以我们需要<code>minus</code>函数让权值变化矩阵中的元素取反。</p>
<p><code>gradient-descent-bprop</code>函数只是简单的初始化了神经网络的权值变化量矩阵，然后将<code>bprop-step-fn</code>当做<code>step</code>参数传入<code>gradient-descent</code>函数，有了<code>gradient-descent-bprop</code>函数我们就可以实现之前定义的抽象协议<code>NeuralNetwork</code>，如下面代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> round-output</span><br><span class="line">  <span class="string">"利用四舍五入返回距离传入浮点数最近的整数"</span></span><br><span class="line">  [output]</span><br><span class="line">  (<span class="name"><span class="builtin-name">mapv</span></span> #(<span class="name">Math/round</span> <span class="comment">^Double</span> %) output))</span><br><span class="line"></span><br><span class="line">(<span class="name"><span class="builtin-name">defrecord</span></span> MultiLayerPerceptron [options]</span><br><span class="line">  NeuralNetwork</span><br><span class="line"></span><br><span class="line">  <span class="comment">;; 计算给定输入的输出值</span></span><br><span class="line">  (<span class="name">run</span> [network inputs]</span><br><span class="line">    (<span class="name"><span class="builtin-name">let</span></span> [weights (<span class="symbol">:weights</span> network)</span><br><span class="line">          input-activations (<span class="name">matrix</span> inputs)]</span><br><span class="line">      (<span class="name">forward-propagate</span> weights input-activations)))</span><br><span class="line"></span><br><span class="line">  <span class="comment">;; 对于给定的输入将输出值映射到一个二值范围内</span></span><br><span class="line">  (<span class="name">run-binary</span> [network inputs]</span><br><span class="line">    (<span class="name">round-output</span> (<span class="name">run</span> network inputs)))</span><br><span class="line"></span><br><span class="line">  <span class="comment">;; 利用样本数据来训练多层感知器神经网络</span></span><br><span class="line">  (<span class="name">train-ann</span> [network samples]</span><br><span class="line">    (<span class="name"><span class="builtin-name">let</span></span> [options         (<span class="symbol">:options</span> network)</span><br><span class="line">          hidden-neurons  (<span class="symbol">:hidden-neurons</span> options)</span><br><span class="line">          epsilon         (<span class="symbol">:weight-epsilon</span> options)</span><br><span class="line">          [first-in</span><br><span class="line">           first-out]     (<span class="name"><span class="builtin-name">first</span></span> samples)</span><br><span class="line">          num-inputs      (<span class="name">length</span> first-in)</span><br><span class="line">          num-outputs     (<span class="name">length</span> first-out)</span><br><span class="line">          sample-matrix   (<span class="name"><span class="builtin-name">map</span></span> #(<span class="name"><span class="builtin-name">list</span></span> (<span class="name">matrix</span> (<span class="name"><span class="builtin-name">first</span></span> %))</span><br><span class="line">                                      (<span class="name">matrix</span> (<span class="name"><span class="builtin-name">second</span></span> %)))</span><br><span class="line">                               samples)</span><br><span class="line">          layer-sizes     (<span class="name"><span class="builtin-name">conj</span></span> (<span class="name"><span class="builtin-name">vec</span></span> (<span class="name"><span class="builtin-name">cons</span></span> num-inputs</span><br><span class="line">                                           hidden-neurons))</span><br><span class="line">                                num-outputs)</span><br><span class="line">          new-weights     (<span class="name">random-initial-weights</span> layer-sizes epsilon)</span><br><span class="line">          network         (<span class="name"><span class="builtin-name">assoc</span></span> network <span class="symbol">:weights</span> new-weights)]</span><br><span class="line">      (<span class="name">gradient-descent-bprop</span> network sample-matrix))))</span><br></pre></td></tr></table></figure>
<p>上面代码定义的<code>MultiLayerPerceptron</code>类型使用<code>gradient-descent-bprop</code>函数来训练一个多层感知器神经网络。<code>train-ann</code>函数首先会从神经网络对象的配置选项中提取出隐含层节点的个数以及常数<script type="math/tex">\epsilon</script>的值，神经网络各个层级的大小也绑定在了<code>layer-sizes</code>变量上面。然后使用<code>random-initial-weights</code>函数来初始化神经网络的权值矩阵，并且使用<code>assoc</code>函数去更新<code>network</code>对象中的权值矩阵。最终调用<code>gradient-descent-bprop</code>函数利用反向传播学习算法去训练神经网络。</p>
<p>使用<code>MultiLayerPerceptron</code>类定义的神经网络对象根据<code>NeuralNetwork</code>协议规定实现了两个其他的函数，<code>run</code>和<code>run-binary</code>。<code>run</code>函数使用<code>forward-propagate</code>函数利用一个训练好的神经网络和一个输入样本来计算得到输出值。<code>run-binary</code>函数只是简单的将<code>run</code>函数计算得到的输出值做了一下四舍五入的操作，从而将输出值映射到了一个二值集合上面。</p>
<p>一个使用<code>MultiLayerPerceptron</code>类定义的神经网络对象还需要一个可以用于描述神经网络所有配置选项值的<code>options</code>对象作为初始化参数。我们可以用下面的代码定义这个配置参数对象：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">def</span></span> default-options</span><br><span class="line">  &#123;<span class="symbol">:max-iters</span> <span class="number">10000</span></span><br><span class="line">   <span class="symbol">:desired-error</span> <span class="number">0.0020</span></span><br><span class="line">   <span class="symbol">:hidden-neurons</span> [<span class="number">3</span>]</span><br><span class="line">   <span class="symbol">:learning-rate</span> <span class="number">0.3</span></span><br><span class="line">   <span class="symbol">:learning-momentum</span> <span class="number">0.01</span></span><br><span class="line">   <span class="symbol">:weight-epsilon</span> <span class="number">5</span>&#125;)</span><br><span class="line"></span><br><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> train [samples]</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [network (<span class="name">MultiLayerPerceptron.</span> default-options)]</span><br><span class="line">    (<span class="name">train-ann</span> network samples)))</span><br></pre></td></tr></table></figure>
<p><code>default-options</code>对象中包含了一系列用于抽象描述神经网络的配置参数，如下所示：</p>
<ul>
<li><code>:max-iters</code>: 这个键确定了梯度下降算法迭代的最大次数。</li>
<li><code>:desired-error</code>: 这个变量确定了神经网络可以接受的<strong>MSE</strong>的阈值。</li>
<li><code>:hidden-neurons</code>: 这个变量确定了神经网络中隐含层节点的个数，<code>[3]</code>表示了只有一个隐含层，并且这个隐含层有三个感知器节点。</li>
<li><code>:learning-rate</code>与<code>:learning-momentum</code>: 这两个键对应的值确定了反向传播学习算法更新权值这一个步骤中的学习速率和学习动量值。</li>
<li><code>:epsilon</code>: 这个变量确定了在<code>random-initial-weights</code>函数中初始化神经网络权值矩阵时用到的常数。</li>
</ul>
<p>我们同样定义了一个简单的帮助函数<code>train</code>，这个函数会创建一个<code>MultiLayerPerceptron</code>类型的神经网络对象并且使用<code>train-ann</code>函数和用<code>samples</code>参数指定的样本数据来训练这个新创建的神经网络对象。现在我们可以用被<code>sample-data</code>变量指定的训练样本集来创建并训练一个人工神经网络了。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">user&gt; (def MLP (train sample-data))</span><br><span class="line">#'user/MLP</span><br></pre></td></tr></table></figure>
<p>然后我们就可以使用这个被训练好的神经网络来针对一些输入值预测输出值了。<code>MLP</code>变量表示的神经网络产生的结果和异或门产生的输出已经非常接近了。</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">user&gt; (run-binary MLP  [0 1])</span><br><span class="line">[<span class="number">1</span>]</span><br><span class="line">user&gt; (run-binary MLP  [1 0])</span><br><span class="line">[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>然而，对于某一些输入来说，训练好的神经网络也可能会输出不正确的结果，如下所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">user&gt; (run-binary MLP  [0 0])</span><br><span class="line">[<span class="number">0</span>]</span><br><span class="line">user&gt; (run-binary MLP  [1 1]) ;; 产生了不正确的输</span><br><span class="line">[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>为了提高神经网络输出值的精度，我们需要实现一些措施。我们可以使用神经网络的权值去正则化计算出来的梯度值，在之前的公式中也提到了带有正则化项的损失函数。加入了正则化项可以显著地提高神经网络预测的正确性。另外我们还可以增大神经网络训练迭代的最大次数。同样的我们还可以通过适当地调整神经网络隐含层的层数，隐含层中节点的个数，学习速率和学习动量等参数来使得这个分类预测算法表现的更加优秀，读者可以自行尝试做这些修改。</p>
<p><strong>Enclog</strong>这个库(<em><a href="https://github.com/jimpil/enclog" target="_blank" rel="noopener">https://github.com/jimpil/enclog</a></em>)对<strong>Encog</strong>这个机器学习和人工神经网络算法库的一个Clojure封装。Encog库(<em><a href="https://github.com/encog" target="_blank" rel="noopener">https://github.com/encog</a></em>)有两个基本的实现，一个是在JVM平台上用java实现的，另一个是在.NET平台上用c#实现的。我们可以使用Enclog库非常轻松地构建人工神经网络来解决监督学习和非监督学习问题。</p>
<blockquote>
<p>要使用Enclog库，需要在Leiningen项目下的<code>project.clj</code>中加入如下所示的依赖代码：<br/><br>[org.encog/encog-core “3.1.0”]<br/><br>[enclog “0.6.3”]<br/><br>注意如上面代码所示Enclog库是依赖于Encog这个Java库的。作为示例，我们需要像下面代码所示那样去修改名字空间的声明从而可以在代码中使用Enclog库中的函数：<br/><br>(ns my-namespace<br/><br>&nbsp;&nbsp;(:use [enclog nnets training]))</p>
</blockquote>
<p>我们可以使用Enclog库中的<code>enclog.nnets</code>名字空间下的<code>neural-pattern</code>和<code>network</code>函数来创建ANN对象。<code>neural-pattern</code>函数被用来确定ANN中的神经网络模型。<code>network</code>函数接受从<code>neural-pattern</code>函数返回的神经网络模型从而创建一个新的ANN对象。我们可以根据具体情况下要使用的神经网络模型向<code>network</code>函数和<code>neural-pattern</code>函数提供一些配置选项值。比如下面的代码就可以定义一个前馈多层感知器神经网络：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">def</span></span> mlp (<span class="name">network</span> (<span class="name">neural-pattern</span> <span class="symbol">:feed-forward</span>)</span><br><span class="line">                  <span class="symbol">:activation</span> <span class="symbol">:sigmoid</span></span><br><span class="line">                  <span class="symbol">:input</span>      <span class="number">2</span></span><br><span class="line">                  <span class="symbol">:output</span>     <span class="number">1</span></span><br><span class="line">                  <span class="symbol">:hidden</span>     [<span class="number">3</span>]))</span><br></pre></td></tr></table></figure>
<p>对于一个前馈神经网络来说，我们可以在<code>network</code>函数中使用<code>:activation</code>这个键去指定激活函数。在我们的示例中，我们使用<code>:sigmoid</code>作为<code>:activation</code>键的值去指定人工神经网络中每一个节点的激活函数为sigmoid函数。我们同样可以用<code>:input</code>，<code>:output</code>和<code>:hidden</code>等键去指定神经网络中输入层，输出层以及隐含层节点的个数。</p>
<p>我们可以使用<code>enclog.training</code>名字空间中的<code>trainer</code>和<code>train</code>函数以及一些样本数据去训练通过<code>network</code>函数创建的ANN对象。在训练神经网络的时候必须将要使用的学习算法指定为第一个参数传给<code>trainer</code>函数。比如要使用反向传播学习算法，那么就用<code>:back-prop</code>关键字来指定。<code>trainer</code>函数会返回一个已经包含一个我们设定的学习算法的ANN对象。然后<code>train</code>函数在根据ANN对象中指定好的学习算法去训练其中的神经网络，这个过程如下面代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> train-network [network data trainer-algo]</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [trainer (<span class="name">trainer</span> trainer-algo</span><br><span class="line">                         <span class="symbol">:network</span> network</span><br><span class="line">                         <span class="symbol">:training-set</span> data)]</span><br><span class="line">    (<span class="name">train</span> trainer <span class="number">0.01</span> <span class="number">1000</span> [])))</span><br></pre></td></tr></table></figure>
<p>上面代码定义的<code>train-network</code>函数需要接受三个参数。第一个参数是利用<code>network</code>函数创建的ANN对象，第二个参数是用来训练神经网络的训练数据集，第三个参数是指定训练神经网络时需要使用的学习算法。在上面的代码中我们利用<code>:network</code>和<code>:training-set</code>这两个键值来确定传给<code>trainer</code>函数的ANN对象和训练数据集。然后<code>train</code>函数再利用确定好的学习算法和样本集来训练神经网络。我们可以在<code>train</code>函数中分别用第一个参数和第二个参数来指定学习算法的期望误差阈值以及最大迭代次数。在上面的例子中，期望误差是<code>0.01</code>，最大的迭代次数是<code>1000</code>。传给<code>train</code>函数的最后一个参数是一个用来指定ANN对象行为的响亮，这里我们不需要指定特定行为，所以传递一个空响亮。<br>利用Enclog库中的<code>data</code>函数，我们可以用来创建用于训练神经网络的训练数据集。比如我们可以用<code>data</code>函数创建一个用于解决逻辑与非门问题的训练数据集，如下面的代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">def</span></span> dataset</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [xor-input [[<span class="number">0.0</span> <span class="number">0.0</span>] [<span class="number">1.0</span> <span class="number">0.0</span>] [<span class="number">0.0</span> <span class="number">1.0</span>] [<span class="number">1.0</span> <span class="number">1.0</span>]]</span><br><span class="line">        xor-ideal [[<span class="number">0.0</span>]     [<span class="number">1.0</span>]     [<span class="number">1.0</span>]     [<span class="number">0.0</span>]]]</span><br><span class="line">        (<span class="name">data</span> <span class="symbol">:basic-dataset</span> xor-input xor-ideal)))</span><br></pre></td></tr></table></figure>
<p><code>data</code>函数需要用数据的类型作为第一个参数，然后将训练样本集的输入值和输出值以向量的形式分别以第二个和第三个参数传入<code>data</code>函数。在我们的例子中，我们会使用<code>:basic-dataset</code>和<code>basic</code>参数。<code>:basic-dataset</code>关键字可以被用来创建训练数据集，<code>:basic</code>关键字怎被用来指定训练完之后使用神经网络进行分类预测时输入值的类型。</p>
<p>使用<code>dataset</code>变量指定的数据集以及<code>train-network</code>函数，我们就可以将上面定义的ANN对象训练成一个与非门了，如下面代码所示：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">user&gt; (def MLP (train-network mlp dataset :back-prop))</span><br><span class="line">Iteration # 1 Error: 26.461526% Target-Error: 1.000000%</span><br><span class="line">Iteration # 2 Error: 25.198031% Target-Error: 1.000000%</span><br><span class="line">Iteration # 3 Error: 25.122343% Target-Error: 1.000000%</span><br><span class="line">Iteration # 4 Error: 25.179218% Target-Error: 1.000000%</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">Iteration # 999 Error: 3.182540% Target-Error: 1.000000%</span><br><span class="line">Iteration # 1,000 Error: 3.166906% Target-Error: 1.000000%</span><br><span class="line">#'user/MLP</span><br></pre></td></tr></table></figure>
<p>根据上面代码的输出，可以看到最终训练完神经网络仍然有<script type="math/tex">3.16\%</script>的误差，而目标误差是<script type="math/tex">1.0\%</script>，解决这个问题可以通过增大跌打次数的方式。我们现在可以利用训练好的神经网络来根据输入值进行输出分类预测值了。为了做到这一点，我们使用Java代码<code>compute</code>和<code>getData</code>方法(<em>在Encog库中</em>)，在clojure代码中使用这两个Java函数需要以<code>.compute</code>和<code>.getData</code>的形式来使用。我们可以定义一个帮助函数去调用<code>.compute</code>函数，从而可以接受一个向量形式的输入值并且得到一个二值化的分类预测输出值，如下面代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> run-network [network input]</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [input-data (<span class="name">data</span> <span class="symbol">:basic</span> input)</span><br><span class="line">        output     (<span class="name">.compute</span> network input-data)</span><br><span class="line">        output-vec (<span class="name">.getData</span> output)]</span><br><span class="line">    (<span class="name">round-output</span> output-vec)))</span><br></pre></td></tr></table></figure>
<p>现在我们可以使用<code>run-network</code>函数和向量形式的输入值来测试训练好的神经网络，如下面代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">user&gt; (run-network MLP [1 1])</span><br><span class="line">[<span class="number">0</span>]</span><br><span class="line">user&gt; (run-network MLP [1 0])</span><br><span class="line">[<span class="number">1</span>]</span><br><span class="line">user&gt; (run-network MLP [0 1])</span><br><span class="line">[<span class="number">1</span>]</span><br><span class="line">user&gt; (run-network MLP [0 0])</span><br><span class="line">[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>从上面的代码中可以看到，用<code>MLP</code>变量表示的训练后的神经网络对于给定输入后的输出的行为和与非门完全吻合，当然适当增大训练时的最大迭代次数也可以增大分类预测的准确率。</p>
<p>总的来说，Enclog这个库提供给我们非常少量但是足够强大的函数来构建人工神经网络模型。在前面的例子中我们探索了前馈多层感知器神经网络模型。这个库同样提供了几个其他的神经网络模型，比如<strong>自适应共振理论神经网络(ART)</strong>，<strong>自组织映射神经网络(SOM)</strong>和<strong>艾尔曼网络</strong>。Enclog库同样允许我们去为特定的神经网络模型中的节点自定义激活函数。对于上面的前馈网络的例子，我们就是用了sigmoid函数。其他一些激活函数例如正弦函数，双曲正切函数，对数函数以及线性函数，也都可以在Enclog库中使用。Enclog库同样支持除反向传播算法以为其他多种可以用于训练神经网络的机器学习算法。</p>
<h2 id="理解递归神经网络"><a href="#理解递归神经网络" class="headerlink" title="理解递归神经网络"></a>理解递归神经网络</h2><p>我们现在将目光转向递归神经网络并且学习一个简单的递归神经网络模型。一个艾尔曼神经网络就是有一个输出层一个输入层一个隐含层以及一个隐含层的简单递归神经网络。当然还有一个额外的一层神经元节点作为<em>反馈层</em>。因为反馈层可以将之前一次迭代时隐含层的输出值作为输入传递给当前迭代时的隐含层节点，所以艾尔曼神经网络可以被用来模拟监督学习与非监督学习问题中的短期记忆效应。Enclog库中已经包好了艾尔曼神经网络的实现，下面的章节将会演示如何使用Enclog库来构建一个艾尔曼神经网络。</p>
<p>艾尔曼神经网络的反馈层接受来自隐含层的加权输出作为输入。在这种方式下面，神经网络可以短暂记忆之前使用隐含层生成的值，并用这些值去影响下一次预测时生成的值。因此，反馈层相当于是神经网络的一个短期记忆。一个艾尔曼神经网络可以用下图来表示：</p>
<center>
  <img src="/images/cljml/chap4/clojureformlchap4pic8.png">
</center>

<p>如上图所示的艾尔曼网络的整体结构酷似前馈多层感知器神经网络。艾尔曼网络只是额外地加上了一层神经元节点作为反馈层。上图所示的艾尔曼网络接受两个输入值，并且最终生成两个输入值。和多层感知器一样输入层和隐含层分别加上一个偏移量输入。隐含层神经元节点的激活值直接喂给两个反馈层的节点<script type="math/tex">c_{1}</script>和<script type="math/tex">c_{2}</script>。然后保存反馈层中得数据值之后会在下一次训练时作为输入传递给隐含层，从而让隐含层节点计算出新的激活值。</p>
<p>我们可以将<code>:elman</code>关键字传给Enclog库中的<code>neural-pattern</code>函数，从而创建一个艾尔曼网络，如下面的代码所示：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">def</span></span> elman-network (<span class="name">network</span> (<span class="name">neural-pattern</span> <span class="symbol">:elman</span>)</span><br><span class="line">                             <span class="symbol">:activation</span> <span class="symbol">:sigmoid</span></span><br><span class="line">                             <span class="symbol">:input</span>      <span class="number">2</span></span><br><span class="line">                             <span class="symbol">:output</span>     <span class="number">1</span></span><br><span class="line">                             <span class="symbol">:hidden</span>     [<span class="number">3</span>]))</span><br></pre></td></tr></table></figure>
<p>为了训练艾尔曼网络，我们需要使用弹性传播算法(更多细节可以参考<em>Empirical Evaluation of the Improved Rprop Learning Algorithm</em>)。这个算法同样可以被用来训练其他Enclog库支持的递归神经网络，有趣的是，弹性传播算法同样可以被用来训练前馈网络。某些情况下弹性传播算法表现出来的学习性能明显好于反向传播算法。尽管详细的解释这个算法已经超出本书的范围了，但是还是鼓励读着去详细地了解这个算法的工作原理，上面提到的那篇论文是一个不错的参考资料。可以用<code>:resilient-prop</code>关键字来指定之前定义过的<code>train-network</code>函数来使用弹性传播学习算法。在下面代码中我们使用<code>train-network</code>函数和<code>dataset</code>变量来训练艾尔曼神经网络：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">user&gt; (def EN (train-network elman-network dataset</span><br><span class="line">:resilient-prop))</span><br><span class="line">Iteration # 1 Error: 26.461526% Target-Error: 1.000000%</span><br><span class="line">Iteration # 2 Error: 25.198031% Target-Error: 1.000000%</span><br><span class="line">Iteration # 3 Error: 25.122343% Target-Error: 1.000000%</span><br><span class="line">Iteration # 4 Error: 25.179218% Target-Error: 1.000000%</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">Iteration # 99 Error: 0.979165% Target-Error: 1.000000%</span><br><span class="line">#'user/EN</span><br></pre></td></tr></table></figure>
<p>如上面代码所示，弹性传播算法相对于反向传播算法达到收敛时所需要的算法迭代次数要少得多。现在我们可以像之前的例子一样将这个训练好的艾尔曼网络当做一个逻辑异或门来使用。</p>
<p>总的来说，递归神经网络以及弹性传播学习训练算法是利用ANNs解决分类和回归问题的另一种有效的途径。</p>
<h2 id="建立自组织神经网络"><a href="#建立自组织神经网络" class="headerlink" title="建立自组织神经网络"></a>建立自组织神经网络</h2><p>自组织神经网络(<strong>SOM</strong>)是另外一种用于解决非监督学习问题的非常有趣的神经网络模型。SOMs在有许多实际的应用，比如手写字识别和图像识别。我们在第七章<em>Clustering Data</em>这一章讨论聚类时也会对SOMs模型进行回顾和复习。</p>
<p>在非监督学习领域中，训练数据集中并没有包含每一个样本的期望输出值，所以神经网络必须靠自己去从输入数据中识别和匹配训练数据中的特定模式。SOMs使用是一种<em>竞争学习</em>的方式，是用于解决非监督学习问题的一类特殊的学习算法，在这种方法中，神经元要相互竞争，只有获胜的一个神经元才能被激活。被激活的神经元将直接影响到神经网络最终的输出值，被激活的神经元也叫做<strong>胜利神经元</strong>。</p>
<p>一个自组织神经网络网络本质上是将一个高维数据映射到一个低维平面上的一个点。我们通过更新输入节点至低维平面上神经元节点的权值的方式来对自组织神经网络进行训练。SOM内部的那个低维平面一般只有一维或者二维，这个平面也成为竞争层。SOM中的神经元节点们可以根据输入值有选择地调整最终输出的模式。当SOM中一个特定的神经元被一个特定的输入模式之后，这个神经元附近的神经元也会对这个特定的输入模式产生一定的兴奋度，并且对这个模式做出相应的调整，这种神经元之间的行为也叫做<strong>横向互动</strong>。当SOM从输入数据中发现了一个特定模式之后，如果有一组具有相似模式的数据输入，SOM可以快速的识别出这个模式。自组织神经网络的结构可以用下图来描述：</p>
<center>
  <img src="/images/cljml/chap4/clojureformlchap4pic9.png">
</center>

<p>如上图所示，一个自组织神经网络由一层输入层和一层竞争层组成。自组织神经网络的竞争层也叫做<strong>特征图</strong>。输入节点将输入值映射到给竞争层的神经元节点上。在竞争层的每一个节点都可以将自己的输出乘上权值以后输入到相邻的节点上。这些权值也叫做特征图的<strong>连接权</strong>。SOM可以通过根据输入值来调整与输入节点相关联的竞争层节点的权重的方式来记住输入数据的模式。</p>
<p>自组织神经网络的自组织行为可以描述为如下的过程：</p>
<ol>
<li>所有的权值都是用随机数进行初始化。</li>
<li>对于每一个输入模式，竞争层的节点会利用一个判别式函数去计算一个值，这些被判别式函数计算出来的值之后会被用来决定哪一个是在竞争学习中胜利的神经元节点。</li>
<li>判别式函数计算出来值最小的结果对应的神经元节点将会被选作胜利节点(也就是输入值向量与该输出神经元节点对应的权值向量代表的高维空间中的点的距离最近)，从输入层节点连接到这个竞争层节点的权值也会进行相应地更新，从而保证在有类似模式输入时，这个节点也最有可能在竞争学习中胜出。</li>
</ol>
<p>为了使得有类似模式输入时，靠近胜利节点的神经元节点利用判别式函数计算出来的值也能减小，也就是这些节点也将自身向着输入模式的方向上进化，与这些节点连接的权值也需要进行更新和修改。因此胜利节点以及它附近的节点在有着相似模式的输入值输入时将会有更大的输出值或者说是激活值。通过指定训练算法中的学习速率可以调节权值的变化量，需要注意的是每一个代表输入模式的输入向量以及每一个输入神经元对应的权值向量都需要归一化，也就是向量的模长必须为1。</p>
<p>假设输入样本值是一个<script type="math/tex">D</script>维数据，上文提到的判别式函数可以被定义为如下的形式：</p>
<script type="math/tex; mode=display">d_{j}\left ( x \right ) = \sum_{i=0}^{D}\left ( x_{i} - w_{ji} \right )^{2}</script><p>在上面的等式中，<script type="math/tex">w_{ji}</script>这一项代表第SOM竞争层中第<script type="math/tex">j</script>个神经元节点的权值向量中的第<script type="math/tex">i</script>个权值。<script type="math/tex">w_{j}</script>这个权值向量的长度和与这个神经元节点相连的输入层节点的个数相同。</p>
<p>一旦我们确定了一个在竞争学习中胜出的节点之后，我们必须选择这个胜利节点附近的神经元节点并且让他们进化。和胜利节点一样，这些选出来的节点也需要被更新权值。有很多方案可以用来选择胜利节点的邻居节点来一起进化，但是方便起见，这里只选择一个临近的节点。</p>
<p>一种最基本的权值更新方案就是让当前胜利节点对应的权值向量向当前的输入向量靠近，也就是让这两个向量表示的高维空间中的点的距离越来越小，这种基本的进化策略可以用如下表达式形式化地表述：</p>
<script type="math/tex; mode=display">w_{j}(t+1) = w_{j}(t) + \Delta \; w_{j}(t) = w_{j}(t) + \alpha(x-w_{j}(t))</script><p>我们可以使用<code>bubble</code>函数，或者<code>radial bias</code>函数来选择胜利节点附近的一组同样需要更新权值的神经元节点(更多信息，可以参考论文<em>Multivariable functional interpolation and adaptive networks</em>)。</p>
<p>我们需要按照下面的步骤来实现学习训练算法，从而训练自组织神经网络：</p>
<ol>
<li>用随机数初始化所有竞争层节点的的权值。</li>
<li>从训练数据集中选出样本数据，作为输入模式。</li>
<li>利用输入模式以及判别式函数来找到胜利节点。</li>
<li>更新胜利节点以及其附近的神经元节点的权值。</li>
<li>迭代2至4步，直到算法收敛。</li>
</ol>
<p>Enclog库已经实现了自组织神经网络以及对应的训练算法。我们可以用如下代码利用Enclog库来创建并且训练一个SOM：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">def</span></span> som (<span class="name">network</span> (<span class="name">neural-pattern</span> <span class="symbol">:som</span>) <span class="symbol">:input</span> <span class="number">4</span> <span class="symbol">:output</span> <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> train-som [data]</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [trainer (<span class="name">trainer</span> <span class="symbol">:basic-som</span> <span class="symbol">:network</span> som</span><br><span class="line">                         <span class="symbol">:training-set</span> data</span><br><span class="line">                         <span class="symbol">:learning-rate</span> <span class="number">0.7</span></span><br><span class="line">                         <span class="symbol">:neighborhood-fn</span> (<span class="name">neighborhood-F</span> <span class="symbol">:single</span>))]</span><br><span class="line">    (<span class="name">train</span> trainer Double/NEGATIVE_INFINITY <span class="number">10</span> [])))</span><br></pre></td></tr></table></figure>
<p>上面代码中的<code>som</code>变量代表一个SOM对象。<code>train-som</code>函数可以被用来训练SOM。<code>:basic-som</code>关键字用来指定训练自组织神经网络的学习训练算法。需要注意的是我们用<code>:learning-rate</code>键来指定学习算法的学习速率是<code>0.7</code>。</p>
<p>上面代码中传递给<code>trainer</code>函数的<code>:neighborhood-fn</code>键是用来指定对于一组输入值来说我们用哪种算法来选取自组织神经网络中与胜利节点一起进化的在胜利节点附近的节点。在代码<code>(neighborhood-F :single)</code>的帮助下，我们指定了只是在胜利节点附近选取一个领域节点的选取算法。当然我们也可以指定其他的领域节点选取算法，比如可以用<code>:bubble</code>关键字来指定使用<code>bubble</code>函数，使用<code>:rbf</code>关键字来指定使用<code>radial basis</code>函数。</p>
<p>我们可以使用<code>train-som</code>函数和一些输入模式来训练自组织神经网络。需要注意的是用来训练SOM的训练数据集不会包含任何输出值。自组织神经网络必须靠自己从输入数据中识别出特定的模式。一旦我们训练好了一个自组织神经网络，我们就可以使用<code>classify</code>这个Java方法来确定输入数据的特征了。在下面代码所示的例子中，我们仅仅提供两个输入模式来训练SOM：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="builtin-name">defn</span></span> train-and-run-som []</span><br><span class="line">  (<span class="name"><span class="builtin-name">let</span></span> [input [[<span class="number">-1.0</span>, <span class="number">-1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span> ]</span><br><span class="line">               [<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">-1.0</span>, <span class="number">-1.0</span>]]</span><br><span class="line">        input-data (<span class="name">data</span> <span class="symbol">:basic-dataset</span> input <span class="literal">nil</span>) <span class="comment">;数据集中没有输出值</span></span><br><span class="line">        SOM        (<span class="name">train-som</span> input-data)</span><br><span class="line">        d1 (<span class="name">data</span> <span class="symbol">:basic</span> (<span class="name"><span class="builtin-name">first</span></span> input))</span><br><span class="line">        d2 (<span class="name">data</span> <span class="symbol">:basic</span> (<span class="name"><span class="builtin-name">second</span></span> input))]</span><br><span class="line">    (<span class="name">println</span> <span class="string">"Pattern 1 class:"</span> (<span class="name">.classify</span> SOM d1))</span><br><span class="line">    (<span class="name">println</span> <span class="string">"Pattern 2 class:"</span> (<span class="name">.classify</span> SOM d2))</span><br><span class="line">    SOM))</span><br></pre></td></tr></table></figure>
<p>我们可以执行上面代码定义的<code>train-and-run-som</code>函数来观察一个自组织神经网络是如何将训练数据集中的两个输入模式识别为两个独立的类别的，代码执行的结果如下所示：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">user&gt; (train-and-run-som)</span><br><span class="line">Iteration # 1 Error: 2.137686% Target-Error: NaN</span><br><span class="line">Iteration # 2 Error: 0.641306% Target-Error: NaN</span><br><span class="line">Iteration # 3 Error: 0.192392% Target-Error: NaN</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">Iteration # 9 Error: 0.000140% Target-Error: NaN</span><br><span class="line">Iteration # 10 Error: 0.000042% Target-Error: NaN</span><br><span class="line">Pattern 1 class: 1</span><br><span class="line">Pattern 2 class: 0</span><br><span class="line">#&lt;SOM org.encog.neural.som.SOM@19a0818&gt;</span><br></pre></td></tr></table></figure>
<p>总的来说，自组织神经网络对于解决非监督学习问题是一个很好用的模型。而且我们还可以借助Enclog库很方便地构建自组织神经网络去对这一类问题建模分析从而解决它们。</p>
<h2 id="本章概要"><a href="#本章概要" class="headerlink" title="本章概要"></a>本章概要</h2><p>在这一章中，我们探索了几个有趣的人工神经网络模型。这些模型可以被用来解决监督学习和非监督学习问题。此外我们还讨论了一些其他的问题，如下所示：</p>
<ul>
<li>了解了人工神经网络的重要性以及其主要类型，例如前馈神经网络和递归神经网络。</li>
<li>学习了多层感知器神经网络和用于训练神经网络的反向传播算法。我们通用使用Clojure和矩阵操作实现了一个简单的反向传播算法。</li>
<li>学习使用了Enclog这个库。我们可以使用这个库来构建神经网络从而对监督学习问题和非监督学习问题进行建模。</li>
<li>介绍了艾尔曼网络，是一种递归神经网络，可以在相对较少的迭代次数下使得训练误差很小从而使学习算法收敛。学习了使用Enclog库来构建和训练一个艾尔曼网络。</li>
<li>介绍了自组织神经网络，可以用于解决非监督学习领域内的问题。然后学习使用Enclog库来构建并且训练一个自组织神经网络。</li>
</ul>

  </div>
</article>



<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="http://zjhmale.github.io" target="_blank" rel="noopener">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="http://github.com/zjhmale" target="_blank" rel="noopener">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#理解非线性回归"><span class="toc-number">1.</span> <span class="toc-text">理解非线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#描述神经网络"><span class="toc-number">2.</span> <span class="toc-text">描述神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#理解多层感知器神经网络"><span class="toc-number">3.</span> <span class="toc-text">理解多层感知器神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#理解反向传播算法"><span class="toc-number">4.</span> <span class="toc-text">理解反向传播算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#理解递归神经网络"><span class="toc-number">5.</span> <span class="toc-text">理解递归神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#建立自组织神经网络"><span class="toc-number">6.</span> <span class="toc-text">建立自组织神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#本章概要"><span class="toc-number">7.</span> <span class="toc-text">本章概要</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/" target="_blank" rel="noopener"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&text=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&title=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&is_video=false&description=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=cljmlchapter4-ann&body=Check out this article: http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&title=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&title=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&title=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&title=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&name=cljmlchapter4-ann&description=" target="_blank" rel="noopener"><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://news.ycombinator.com/submitlink?u=http://yoursite.com/2014/08/11/2014-08-11-cljmlchapter4-ann/&t=cljmlchapter4-ann" target="_blank" rel="noopener"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2013-2020
    Zheng Jihui
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="http://zjhmale.github.io" target="_blank" rel="noopener">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="http://github.com/zjhmale" target="_blank" rel="noopener">Projects</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments --><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>
</html>
